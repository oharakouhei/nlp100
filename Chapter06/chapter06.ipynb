{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章: 英語テキストの処理\n",
    "英語のテキスト（[nlp.txt](http://www.cl.ecei.tohoku.ac.jp/nlp100/data/nlp.txt)）に対して，以下の処理を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50. 文区切り\n",
    "(. or ; or : or ? or !) → 空白文字 → 英大文字というパターンを文の区切りと見なし，入力された文書を1行1文の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "\n",
    "class Main(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # [.;:?!]\\s+で区切れば済む話だが、題意に忠実に\n",
    "    def generate_sentences(self):\n",
    "        with open(\"source/nlp.txt\", \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # re.split('[.;:?!]\\s+([A-Z])', text)では、大文字まで区切ってしまうので、\n",
    "        # 後から大文字はmergeする\n",
    "        sentences = []\n",
    "        uppercase_letter = ''\n",
    "        for sentence in re.split('[.;:?!]\\s+([A-Z])', text):\n",
    "            if re.match('^[A-Z]$', sentence):\n",
    "                uppercase_letter = sentence\n",
    "            else:\n",
    "                sentences.append(uppercase_letter + sentence)\n",
    "                uppercase_letter = ''\n",
    "                \n",
    "        return sentences\n",
    "    \n",
    "    def solve(self):\n",
    "        sentences = []\n",
    "        sentences = self.generate_sentences()\n",
    "        with open(\"source/nlp.txt.pickle\", \"wb\") as w:\n",
    "            pickle.dump(sentences, w)\n",
    "            \n",
    "        for sentence in sentences:\n",
    "            print(sentence)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 51. 単語の切り出し\n",
    "空白を単語の区切りとみなし，50の出力を入力として受け取り，1行1単語の形式で出力せよ．ただし，文の終端では空行を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self):\n",
    "        with open(\"source/nlp.txt.pickle\", \"rb\") as f:\n",
    "            sentences = pickle.load(f)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                print(word)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 52. ステミング\n",
    "51の出力を入力として受け取り，Porterのステミングアルゴリズムを適用し，単語と語幹をタブ区切り形式で出力せよ． Pythonでは，Porterのステミングアルゴリズムの実装として[stemming](https://pypi.python.org/pypi/stemming)モジュールを利用するとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self):\n",
    "        with open(\"source/nlp.txt.pickle\", \"rb\") as f:\n",
    "            sentences = pickle.load(f)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                print(stem(word))\n",
    "            print(\"\\n\")\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 53. Tokenization\n",
    "[Stanford Core NLP](http://stanfordnlp.github.io/CoreNLP/)を用い，入力テキストの解析結果をXML形式で得よ．また，このXMLファイルを読み込み，入力テキストを1行1単語の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 54. 品詞タグ付け\n",
    "Stanford Core NLPの解析結果XMLを読み込み，単語，レンマ，品詞をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 55. 固有表現抽出\n",
    "入力文中の人名をすべて抜き出せ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 56. 共参照解析\n",
    "Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 57. 係り受け解析\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 58. タプルの抽出\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）に基づき，「主語 述語 目的語」の組をタブ区切り形式で出力せよ．ただし，主語，述語，目的語の定義は以下を参考にせよ．\n",
    "\n",
    "* 述語: nsubj関係とdobj関係の子（dependant）を持つ単語\n",
    "* 主語: 述語からnsubj関係にある子（dependent）\n",
    "* 目的語: 述語からdobj関係にある子（dependent）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 59. S式の解析\n",
    "Stanford Core NLPの句構造解析の結果（S式）を読み込み，文中のすべての名詞句（NP）を表示せよ．入れ子になっている名詞句もすべて表示すること．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
