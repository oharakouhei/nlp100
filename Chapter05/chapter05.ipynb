{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 係り受け解析\n",
    "\n",
    "夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](http://www.cl.ecei.tohoku.ac.jp/nlp100/data/neko.txt)）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neko.txt.cabochaの作成\n",
    "形態素も出すような形式にするためにオプション -f1 をつける\n",
    "```\n",
    "$ cabocha -f1 neko.txt >> neko.txt.cabocha\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Morph(object):\n",
    "\n",
    "    def __init__(self, surface=None, base=None, pos=None, pos1=None):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<surface: {0} base: {1}, pos: {2}, pos1: {3}>'.format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def solve(self):\n",
    "        with open('source/neko.txt.cabocha', 'r') as f:\n",
    "            morphologies = []\n",
    "            sentence = []\n",
    "            for line in f.readlines():\n",
    "                if line.strip() == 'EOS':\n",
    "                    if len(sentence) > 0:\n",
    "                        morphologies.append(sentence)\n",
    "                        sentence = []\n",
    "                elif not line.startswith('*'):\n",
    "                    surface, result = line.split(\"\\t\")\n",
    "                    results = result.split(',')\n",
    "                    sentence.append(Morph(surface, results[6], results[0], results[1]))\n",
    "            print(morphologies[2])\n",
    "        \n",
    "        return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Chunk(object):\n",
    "    def __init__(self, morphs=None, dst=None, srcs=None):\n",
    "        if morphs is None: morphs = []\n",
    "        if srcs is None: srcs = []\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "        \n",
    "    def get_text(self):\n",
    "        return ''.join([m.surface for m in self.morphs if m.pos != '記号'])\n",
    "\n",
    "    def append(self, morph):\n",
    "        self.morphs.append(morph)\n",
    "                \n",
    "    def __str__(self):\n",
    "        return '<text: {0}, srcs: {1}, dst: {2}>'.format(self.get_text(), str(self.srcs), self.dst)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ParseCabochaTextToChunks(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def parse(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            sentences = [] # sentenceの集合\n",
    "            sentence = [] # 1文単位で各要素にchunkを保存するリスト\n",
    "            chunk = None\n",
    "            \n",
    "            for line in f.readlines():\n",
    "                if line.strip() == 'EOS':\n",
    "                    self.set_srcs(sentence)\n",
    "                    # self.reset_dst(sentence) # 不要だと考え一度コメントアウト\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []   \n",
    "                else:\n",
    "                    if line.startswith('*'):\n",
    "                        chunk = self.parse_chunk_header(line)\n",
    "                        sentence.append(chunk)\n",
    "                    else:\n",
    "                        morph = self.parse_morph(line)\n",
    "                        chunk.append(morph)\n",
    "                        \n",
    "        return sentences\n",
    "\n",
    "    # EOS時に実行する\n",
    "    def set_srcs(self, sentence):\n",
    "        for index, chunk in enumerate(sentence):\n",
    "            # dstが存在していれば\n",
    "            if chunk.dst != -1:\n",
    "                sentence[chunk.dst].srcs.append(index)\n",
    "     \n",
    "#     # EOS時に実行する\n",
    "#     # 1文の最後のchunkのdstを-1からNoneに設定する\n",
    "#     def reset_dst(self, sentence):\n",
    "#         if len(sentence) > 0:\n",
    "#             sentence[-1].dst = None\n",
    "    \n",
    "    # 1文の最初に実行\n",
    "    # * 2 4D 0/1 1.227694 等の行の、4Dから4を取り出しdstとして保存\n",
    "    def parse_chunk_header(self, line):\n",
    "        chunk = Chunk()\n",
    "        chunk.dst = int(line.split()[2][:-1])\n",
    "        return chunk\n",
    "\n",
    "    # 形態素の行で実行\n",
    "    # 形態素解析\n",
    "    def parse_morph(self, line):\n",
    "        surface, result = line.split(\"\\t\")\n",
    "        results = result.split(',')\n",
    "        morph = Morph(surface, results[6], results[0], results[1])\n",
    "        return morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Main(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self):\n",
    "        pcttc = ParseCabochaTextToChunks()\n",
    "        sentences = pcttc.parse('source/neko.txt.cabocha')\n",
    "        with open('source/neko.txt.cabocha.pickle', 'wb') as w:\n",
    "            pickle.dump(sentences, w) \n",
    "        print(text[7])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self):\n",
    "        with open('source/neko.txt.cabocha.pickle', 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for chunk in sentence:\n",
    "                if -1 != chunk.dst:\n",
    "                    print('{0}\\t{1}'.format(chunk.get_text(), sentence[chunk.dst].get_text()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self):\n",
    "        with open('source/neko.txt.cabocha.pickle', 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for chunk in sentence:\n",
    "                if -1 != chunk.dst:\n",
    "                    if '名詞' in [m.pos for m in chunk.morphs] and '動詞' in [m.pos for m in sentence[chunk.dst].morphs]:\n",
    "                        print('{0}\\t{1}'.format(chunk.get_text(), sentence[chunk.dst].get_text()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木を[DOT言語](https://ja.wikipedia.org/wiki/DOT%E8%A8%80%E8%AA%9E)に変換し，[Graphviz](http://www.graphviz.org/)を用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，[pydot](https://github.com/erocarrera/pydot)を使うとよい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pydot import pydot\n",
    "# 以下画像表示用\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self):\n",
    "        with open('source/neko.txt.cabocha.pickle', 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "        \n",
    "        # 8行目だけ可視化\n",
    "        sentence = sentences[7]\n",
    "        edges = []\n",
    "        for chunk in sentence:\n",
    "            if -1 != chunk.dst:\n",
    "                edges.append((chunk.get_text(), sentence[chunk.dst].get_text()))\n",
    "       \n",
    "        g = pydot.graph_from_edges(edges)\n",
    "        g.write_jpeg('44.png', prog='dot')\n",
    "        im = Image.open(\"44.png\", \"r\")\n",
    "        plt.imshow(np.array(im))\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "* 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "* 述語に係る助詞を格とする\n",
    "* 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で\n",
    "見る    は を\n",
    "```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self):\n",
    "        with open('source/neko.txt.cabocha.pickle', 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "        \n",
    "        verbs = dict()\n",
    "        with open(\"45.txt\", \"w\") as w:\n",
    "            for sentence in sentences:\n",
    "                for chunk in sentence:\n",
    "                    has_verb = False\n",
    "                    has_joshi = False\n",
    "                    base = \"\"\n",
    "                    # chunk内が述語があるか走査\n",
    "                    for m in chunk.morphs:\n",
    "                        if '動詞' == m.pos:\n",
    "                            base = m.base\n",
    "                            has_verb = True\n",
    "                    # 述語があれば、述語に係る助詞（格）を取得\n",
    "                    if has_verb:\n",
    "                        joshis = []\n",
    "                        for src in chunk.srcs:\n",
    "                            for sm in sentence[src].morphs:\n",
    "                                if '助詞' == sm.pos:\n",
    "                                    joshis.append(sm.base)\n",
    "                                    has_joshi = True # srcsのchunkのうち一つでも助詞を含んでいれば、Trueにして良い\n",
    "                        if has_joshi:\n",
    "                            w.write(\"{0}\\t{1}\\n\".format(base, \" \".join(joshis)))\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# python内でcountもしてしまう場合\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self):\n",
    "        with open('source/neko.txt.cabocha.pickle', 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "        \n",
    "        verbs = dict()\n",
    "        for sentence in sentences:\n",
    "            for chunk in sentence:\n",
    "                has_verb = False\n",
    "                base = \"\"\n",
    "                # chunk内が述語があるか走査\n",
    "                for m in chunk.morphs:\n",
    "                    if '動詞' == m.pos:\n",
    "                        base = m.base\n",
    "                        has_verb = True\n",
    "                # 述語があれば、述語に係る助詞（格）を取得\n",
    "                if has_verb:\n",
    "                    for src in chunk.srcs:\n",
    "                        for sm in sentence[src].morphs:\n",
    "                            if '助詞' == sm.pos:\n",
    "                                if base not in verbs:\n",
    "                                    verbs[base] = defaultdict(int)\n",
    "                                verbs[base][sm.base] += 1\n",
    "                                \n",
    "\n",
    "        for verb, joshi_dict in verbs.items():\n",
    "            joshi_group = \"\"\n",
    "            for joshi, count in joshi_dict.items():\n",
    "                # countも出力\n",
    "                joshi_group += joshi +\"(\"+ str(count) + \") \"\n",
    "            print(\"{0}\\t{1}\".format(verb, joshi_group[0:-1]))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確認するUNIXコマンド\n",
    "```\n",
    "$ sort 45.txt | grep \"^する\\|見る\\|与える\" | uniq -c | sort -r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "* 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "* 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self):\n",
    "        with open('source/neko.txt.cabocha.pickle', 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "        \n",
    "        verbs = dict()\n",
    "#         for sentence in sentences:\n",
    "        sentence = sentences[7]\n",
    "        for chunk in sentence:\n",
    "            has_verb = False\n",
    "            has_joshi = False\n",
    "            base = \"\"\n",
    "            # chunk内が述語があるか走査\n",
    "            for m in chunk.morphs:\n",
    "                if '動詞' == m.pos:\n",
    "                    base = m.base\n",
    "                    has_verb = True\n",
    "            # 述語があれば、述語に係る助詞（格）を取得\n",
    "            if has_verb:\n",
    "                joshis = []\n",
    "                joshi_chunks = []\n",
    "                for src in chunk.srcs:\n",
    "                    for sm in sentence[src].morphs:\n",
    "                        if '助詞' == sm.pos:\n",
    "                            joshis.append(sm.base)\n",
    "                            joshi_chunks.append(sentence[src].get_text())\n",
    "                            has_joshi = True # srcsのchunkのうち一つでも助詞を含んでいれば、Trueにして良い\n",
    "                if has_joshi:\n",
    "                    print(\"{0}\\t{1}\\t{2}\".format(base, \" \".join(joshis), \" \".join(joshi_chunks)))\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "* 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "* 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "* 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "* 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "```\n",
    "\n",
    "* コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "* コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def solve(self):\n",
    "        with open('source/neko.txt.cabocha.pickle', 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "        \n",
    "        verbs = dict()\n",
    "#         for sentence in sentences:\n",
    "        sentence = sentences[948]\n",
    "        for chunk in sentence:\n",
    "            has_verb = False\n",
    "            has_joshi = False\n",
    "            base = \"\"\n",
    "            # chunk内に述語があるか走査\n",
    "            for m in chunk.morphs:\n",
    "                if '動詞' == m.pos:\n",
    "                    base = m.base\n",
    "                    has_verb = True\n",
    "            # 述語があれば、述語に係る助詞（格）を取得\n",
    "            if has_verb:\n",
    "                joshis = []\n",
    "                joshi_chunks = []\n",
    "                for src in chunk.srcs:\n",
    "                    if \"サ変接続\" in [m.pos1 for m in sentence[src].morphs] and \"を\" in [m.surface for m in sentence[src].morphs]:\n",
    "                        base = sentence[src].get_text() + sentence[sentence[src].dst].get_text() # 述語の更新\n",
    "                        continue\n",
    "                    # 述語に係る助詞を抽出する\n",
    "                    for sm in sentence[src].morphs:\n",
    "                        if '助詞' == sm.pos:\n",
    "                            joshis.append(sm.base)\n",
    "                            joshi_chunks.append(sentence[src].get_text())\n",
    "                            has_joshi = True # srcsのchunkのうち一つでも助詞を含んでいれば、Trueにして良い\n",
    "                if has_joshi:\n",
    "                    print(\"{0}\\t{1}\\t{2}\".format(base, \" \".join(joshis), \" \".join(joshi_chunks)))\n",
    "                    #=> 返事をする\tさ と は に\t及ばんさと 及ばんさと 主人は 手紙に\n",
    "                    # 終助詞の「さ」も入っているのはおかしい（？） 「述語に係る助詞」は何助詞があるのか\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "* 各文節は（表層形の）形態素列で表現する\n",
    "* パスの開始文節から終了文節に至るまで，各文節の表現を\"->\"で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 再帰関数を使うver\n",
    "import pickle\n",
    "\n",
    "SENTENCE_INDEX = 7\n",
    "\n",
    "class Main(object):\n",
    "    def __init__(self):\n",
    "        with open('source/neko.txt.cabocha.pickle', 'rb') as f:\n",
    "            self.sentences = pickle.load(f)\n",
    "    \n",
    "    # 再帰\n",
    "    def get_paths(self, sentence_index, chunk_index, paths = None):\n",
    "        if paths is None: paths = []\n",
    "        chunk = self.sentences[sentence_index][chunk_index]\n",
    "        paths.append(chunk.get_text())\n",
    "        if -1 != chunk.dst:\n",
    "            paths = self.get_paths(sentence_index, chunk.dst, paths)\n",
    "        return paths\n",
    "        \n",
    "    def solve(self):        \n",
    "        for chunk_index, chunk in enumerate(self.sentences[SENTENCE_INDEX]):\n",
    "            for m in chunk.morphs:\n",
    "                if '名詞' == m.pos:\n",
    "                    paths = self.get_paths(SENTENCE_INDEX, chunk_index)\n",
    "                    print(\" -> \".join(paths))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 再帰関数を使わないver\n",
    "import pickle\n",
    "\n",
    "class Main(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def solve(self):\n",
    "        with open('source/neko.txt.cabocha.pickle', 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "\n",
    "        sentence = sentences[7]\n",
    "        for index, chunk in enumerate(sentence):\n",
    "            if '名詞' in [m.pos for m in chunk.morphs]:\n",
    "                paths = []\n",
    "                node = index\n",
    "                while True:\n",
    "                    paths.append(sentence[node].get_text())\n",
    "                    node = sentence[node].dst\n",
    "                    if -1 == node:\n",
    "                        break\n",
    "                print(\" -> \".join(paths))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がiとj（i<j）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "* 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"->\"で連結して表現する\n",
    "* 文節iとjに含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "* 文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n",
    "* 上記以外で，文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス，文節kの内容を\"|\"で連結して表示．簡易に示すと，ik間のパス|jk間のパス|k\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た            # 「吾輩は」と「ここで」のペア\n",
    "Xは | Yという -> ものを | 見た                                   # 「吾輩は」と「人間という」のペア\n",
    "Xは | Yを | 見た                                                  # 「吾輩は」と「ものを」のペア\n",
    "Xで -> 始めて -> Yを                                             # 「ここで」と「人間という」のペア\n",
    "Xで -> 始めて -> 人間という -> Yを                              # 「ここで」と「ものを」のペア\n",
    "Xという -> Yを                                                   # 「人間という」と「ものを」のペア\n",
    "```\n",
    "この8文目には名詞句が4つ存在するので，それらの組み合わせは、4C2 = 6通りある．出力が6種になるのはこのためである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "SENTENCE_INDEX = 7\n",
    "\n",
    "class Main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def solve(self):\n",
    "        with open('source/neko.txt.cabocha.pickle', 'rb') as f:\n",
    "            sentences = pickle.load(f)\n",
    "\n",
    "        sentence = sentences[SENTENCE_INDEX]\n",
    "        \n",
    "        for i in range(len(sentence)-1):\n",
    "            if '名詞' in [m.pos for m in sentence[i].morphs]:\n",
    "                for j in range(i+1, len(sentence)):\n",
    "                    if '名詞' in [m.pos for m in sentence[j].morphs]:\n",
    "                        # Get indices of morphs to be replaced by 'X' or 'Y'\n",
    "                        # XまたはYと置き換える形態素indexを取得\n",
    "                        index_x = []\n",
    "                        index_x_flag = False\n",
    "                        for index, m in enumerate(sentence[i].morphs):\n",
    "                            if m.pos == '名詞':\n",
    "                                index_x_flag = True\n",
    "                                index_x.append(index)\n",
    "                            else:\n",
    "                                if index_x_flag:\n",
    "                                    break\n",
    "                        index_y = []\n",
    "                        index_y_flag = False\n",
    "                        for index, m in enumerate(sentence[j].morphs):\n",
    "                            if m.pos == '名詞':\n",
    "                                index_y_flag = True\n",
    "                                index_y.append(index)\n",
    "                            else:\n",
    "                                if index_y_flag:\n",
    "                                    break\n",
    "\n",
    "                        # Get paths of syntax tree\n",
    "                        # それぞれの名詞から根へのパスを取得\n",
    "                        path_i = []\n",
    "                        path_j = []\n",
    "\n",
    "                        node = i\n",
    "                        while True:\n",
    "                            path_i.append(node)\n",
    "                            node = sentence[node].dst\n",
    "                            if -1 == node:\n",
    "                                break\n",
    "                        node = j\n",
    "                        while True:\n",
    "                            path_j.append(node)\n",
    "                            node = sentence[node].dst\n",
    "                            if -1 == node:\n",
    "                                break\n",
    "\n",
    "                        # Display syntax tree\n",
    "                        if set(path_i) >= set(path_j):\n",
    "                            path_texts = self.get_replaced_texts(sentence, sorted(list(set(path_i) - set(path_j)) + [j]), i, j, index_x, index_y)\n",
    "                            print(' -> '.join(path_texts))\n",
    "                        else:\n",
    "                            path_i_only_texts = self.get_replaced_texts(sentence, sorted(list(set(path_i) - set(path_j))), i, j, index_x, index_y)\n",
    "                            path_j_only_texts = self.get_replaced_texts(sentence, sorted(list(set(path_j) - set(path_i))), i, j, index_x, index_y)\n",
    "                            path_common_texts = self.get_replaced_texts(sentence, sorted(list(set(path_i) & set(path_j))), i, j, index_x, index_y)                            \n",
    "                            print('{0} | {1} | {2}'.format(' -> '.join(path_i_only_texts), ' -> '.join(path_j_only_texts), ' -> '.join(path_common_texts)))\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_replaced_texts(self, sentence, path, i, j, replaced_morphs_i, replaced_morphs_j):\n",
    "        texts = []\n",
    "        for index in path:\n",
    "            chunk_text = ''\n",
    "            if index == i:\n",
    "                for index_i, m in enumerate(sentence[index].morphs):\n",
    "                    if index_i == replaced_morphs_i[0]:\n",
    "                        chunk_text += 'X'\n",
    "                    elif index_i in replaced_morphs_i:\n",
    "                        # 名詞の連接の場合（例：「二三ページを」）は、「Xページを」などとせず、「Xを」とする\n",
    "                        pass\n",
    "                    else:\n",
    "                        chunk_text += m.surface\n",
    "            elif index == j:\n",
    "                for index_j, m in enumerate(sentence[index].morphs):\n",
    "                    if index_j == replaced_morphs_j[0]:\n",
    "                        chunk_text += 'Y'\n",
    "                    elif index_j in replaced_morphs_j:\n",
    "                        pass\n",
    "                    else:\n",
    "                        chunk_text += m.surface\n",
    "            else:\n",
    "                chunk_text += sentence[index].get_text()\n",
    "            texts.append(chunk_text)\n",
    "        return texts\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    m = Main()\n",
    "    m.solve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
